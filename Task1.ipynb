{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "In this task, we need to attempt to create a Deep Learning model that, given a sequence of images of Hurricanes, it will try to predict how the hurricane will be like at a future time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, BatchSampler, DataLoader, Sampler\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device(device=\"cpu\", idx=0):\n",
    "    if device != \"cpu\":\n",
    "        if torch.cuda.device_count() > idx and torch.cuda.is_available():\n",
    "            print(\"Cuda installed! Running on GPU {} {}!\".format(idx, torch.cuda.get_device_name(idx)))\n",
    "            device=\"cuda:{}\".format(idx)\n",
    "        elif torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "            print(\"Cuda installed but only {} GPU(s) available! Running on GPU 0 {}!\".format(torch.cuda.device_count(), torch.cuda.get_device_name()))\n",
    "            device=\"cuda:0\"\n",
    "        else:\n",
    "            device=\"cpu\"\n",
    "            print(\"No GPU available! Running on CPU\")\n",
    "    return device\n",
    "\n",
    "device = set_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define the dataset. In the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedStormImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.storms = []\n",
    "        self.index_map = []\n",
    "\n",
    "        for storm_dir in sorted(os.listdir(root_dir)):\n",
    "            storm_path = os.path.join(root_dir, storm_dir)\n",
    "            if os.path.isdir(storm_path):\n",
    "                storm_data = []\n",
    "                for image_file in sorted(os.listdir(storm_path)):\n",
    "                    if image_file.endswith('.jpg'):\n",
    "                        image_path = os.path.join(storm_path, image_file)\n",
    "                        file_stem = image_file.split('.')[0]\n",
    "                        features_json_path = os.path.join(storm_path, file_stem + '_features.json')\n",
    "                        with open(features_json_path, 'r') as f:\n",
    "                            features = json.load(f)\n",
    "                        storm_data.append((image_path, features))\n",
    "                self.storms.append(storm_data)\n",
    "                self.index_map.extend([(len(self.storms) - 1, i) for i in range(len(storm_data))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(storm) for storm in self.storms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            # Convert slice to a range of indices\n",
    "            idx = range(*idx.indices(len(self)))  # Adjusts slice to fit dataset length\n",
    "\n",
    "        if isinstance(idx, range) or isinstance(idx, list):\n",
    "            images = []\n",
    "            features = []\n",
    "            for i in idx:\n",
    "                storm_idx, local_idx = self.index_map[i]\n",
    "                img_path, feature = self.storms[storm_idx][local_idx]\n",
    "                image = Image.open(img_path).convert('L')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                images.append(image)\n",
    "                features.append(feature)\n",
    "            return torch.stack(images), features\n",
    "        else:\n",
    "            storm_idx, local_idx = self.index_map[idx]\n",
    "            img_path, feature = self.storms[storm_idx][local_idx]\n",
    "            image = Image.open(img_path).convert('L')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, feature\n",
    "\n",
    "    def get_storm_sequence(self, storm_idx, seq_start, seq_length):\n",
    "        # Retrieve a sequence of images from a specific storm\n",
    "        images = []\n",
    "        features = []\n",
    "        for i in range(seq_start, seq_start + seq_length):\n",
    "            img_path, feature = self.storms[storm_idx][i]\n",
    "            image = Image.open(img_path).convert('L')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            images.append(image)\n",
    "            features.append(feature)\n",
    "        return torch.stack(images), features  # Stack images to create a sequence tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2471, 0.3373, 0.4314,  ..., 0.0431, 0.0431, 0.0392],\n",
       "          [0.2431, 0.3255, 0.4118,  ..., 0.0431, 0.0431, 0.0392],\n",
       "          [0.2863, 0.3569, 0.4235,  ..., 0.0431, 0.0431, 0.0392],\n",
       "          ...,\n",
       "          [0.0588, 0.0627, 0.0667,  ..., 0.1647, 0.1843, 0.2039],\n",
       "          [0.0627, 0.0667, 0.0706,  ..., 0.1725, 0.1882, 0.2039],\n",
       "          [0.0588, 0.0667, 0.0706,  ..., 0.1647, 0.1804, 0.1922]]]),\n",
       " {'storm_id': 'bkh', 'relative_time': '0', 'ocean': '1'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what an element in the image dataset looks like.\n",
    "dataset = GroupedStormImageDataset(root_dir='./Selected_Storms_curated_to_zip', transform=ToTensor())\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our storm image dataset, each storm represents a different event with a series of images. We need to ensure that batches of data fed into the model are coherent and respect the boundaries of these events to ensure the model learns how a specific storm behaves. So, in order to not mix events, we use:\n",
    "\n",
    "**GroupedStormSampler**\n",
    "\n",
    "This custom sampler is designed to iterate over the dataset while maintaining the grouping by storm events. It ensures that the model sees all images from one storm before moving to the next, respecting the grouping in the data (the individual storms).\n",
    "\n",
    "**GroupedBatchSampler**\n",
    "\n",
    "Building on the GroupedStormSampler, the GroupedBatchSampler takes this a step further by making sure that each batch of data not only comes from the same storm but also follows a specified batch size (at most). This sampler respects the boundaries of each storm, ensuring that no batch contains data from two different storms. If we are at the boundary of two different storms, it will yield a batch size smaller than the maximum batch size.\n",
    "\n",
    "See below for an example that showcases the use of these two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedStormSampler(Sampler):\n",
    "    \"\"\"\n",
    "    A custom sampler for iterating over items in a dataset grouped by storm events,\n",
    "    ensuring that each iteration step progresses through a single storm's data.\n",
    "\n",
    "    Attributes:\n",
    "        data_source (Dataset): The dataset to sample from, expected to have a 'storms' attribute\n",
    "                               that contains grouped data per storm.\n",
    "        indices (list): A list of tuples where each tuple contains the start and end indices\n",
    "                        for samples belonging to the same storm.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "        self.indices = [] # List to hold start and end indices for each storm's data\n",
    "        idx = 0\n",
    "        # Generate start and end indices for each storm\n",
    "        for storm in self.data_source.storms:\n",
    "            self.indices.append((idx, idx + len(storm))) # Append a tuple with start and end indices\n",
    "            idx += len(storm) # Update index to the start of the next storm\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Provides an iterator over the dataset indices, grouped by storm.\n",
    "\n",
    "        Yields:\n",
    "            int: The next index in the dataset, progressing through each storm's data sequentially.\n",
    "        \"\"\"\n",
    "        for start, end in self.indices:\n",
    "            yield from range(start, end) # Yield indices for the current storm\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(storm) for storm in self.data_source.storms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StormBatchSampler(Sampler):\n",
    "    def __init__(self, data_source, sequence_length, batch_size, stride=1, drop_last=True):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for storm_idx, storm_data in enumerate(self.data_source.storms):\n",
    "            # Start from the beginning of each storm\n",
    "            seq_start = 0\n",
    "            while seq_start + self.batch_size <= len(storm_data):\n",
    "                # Find the global index for the start of this sequence\n",
    "                global_start_index = next((i for i, (s_idx, l_idx) in enumerate(self.data_source.index_map) if s_idx == storm_idx and l_idx == seq_start), None)\n",
    "                if global_start_index is not None:\n",
    "                    # Collect the global indices for the entire sequence\n",
    "                    sequence_indices = list(range(global_start_index, global_start_index + self.batch_size))\n",
    "                    batch.append(sequence_indices)\n",
    "\n",
    "                    if len(batch) == self.sequence_length:\n",
    "                        yield batch\n",
    "                        batch = []\n",
    "\n",
    "                # Move to the start of the next sequence within the storm, based on the stride\n",
    "                seq_start += self.stride\n",
    "\n",
    "        # Handle the last batch if drop_last is False\n",
    "        if not self.drop_last and len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Length calculation needs to be implemented based on your dataset's structure\n",
    "        raise NotImplementedError(\"Length calculation needs to be implemented.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6, 1, 366, 366])\n",
      "[{'storm_id': ['blq', 'blq', 'dzw', 'dzw', 'dzw'], 'relative_time': ['655198', '658801', '0', '1799', '3600'], 'ocean': ['1', '1', '1', '1', '1']}, {'storm_id': ['blq', 'blq', 'dzw', 'dzw', 'dzw'], 'relative_time': ['658801', '660600', '1799', '3600', '5399'], 'ocean': ['1', '1', '1', '1', '1']}, {'storm_id': ['blq', 'blq', 'dzw', 'dzw', 'dzw'], 'relative_time': ['660600', '662400', '3600', '5399', '7200'], 'ocean': ['1', '1', '1', '1', '1']}, {'storm_id': ['blq', 'blq', 'dzw', 'dzw', 'dzw'], 'relative_time': ['662400', '664200', '5399', '7200', '10801'], 'ocean': ['1', '1', '1', '1', '1']}, {'storm_id': ['blq', 'blq', 'dzw', 'dzw', 'dzw'], 'relative_time': ['664200', '666000', '7200', '10801', '12601'], 'ocean': ['1', '1', '1', '1', '1']}, {'storm_id': ['blq', 'blq', 'dzw', 'dzw', 'dzw'], 'relative_time': ['666000', '669600', '10801', '12601', '14401'], 'ocean': ['1', '1', '1', '1', '1']}]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "sequence_length = 5  # Length of each sequence\n",
    "batch_size = 6  # Number of sequences per batch\n",
    "stride = 1  # Overlap between sequences\n",
    "storm_sampler = GroupedStormSampler(dataset)\n",
    "\n",
    "sequential_batch_sampler = StormBatchSampler(\n",
    "    data_source=dataset,\n",
    "    sequence_length=sequence_length,\n",
    "    batch_size=batch_size,\n",
    "    stride=stride,\n",
    "    drop_last=True  # You can choose to drop the last incomplete batch if desired\n",
    ")\n",
    "\n",
    "storm_data_loader = DataLoader(dataset, batch_sampler=sequential_batch_sampler)\n",
    "\n",
    "i = 0\n",
    "for batch in storm_data_loader:\n",
    "    if i == 130:\n",
    "        images, data = batch\n",
    "        print(images.shape)  # Should print torch.Size([M, N, C, H, W])\n",
    "        print(data)\n",
    "        break  # To only check the first batch\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvTLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels=8, kernel_size=3, output_channels=1, cuda_flag=False):\n",
    "        super(ConvTLSTMCell, self).__init__()  # Fixed __init__ typo\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.cuda_flag = cuda_flag  # to device instead?\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.W_i = nn.Conv2d(input_channels + hidden_channels, hidden_channels, kernel_size, padding=self.padding)\n",
    "        self.W_f = nn.Conv2d(input_channels + hidden_channels, hidden_channels, kernel_size, padding=self.padding)\n",
    "        self.W_o = nn.Conv2d(input_channels + hidden_channels, hidden_channels, kernel_size, padding=self.padding)\n",
    "        self.W_c = nn.Conv2d(input_channels + hidden_channels, hidden_channels, kernel_size, padding=self.padding)\n",
    "        \n",
    "        # Decay parameters for short-term memory\n",
    "        self.W_d = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, padding=self.padding)\n",
    "\n",
    "        # Output transformation layer\n",
    "        self.output_transform = nn.Conv2d(hidden_channels, output_channels, kernel_size=1)  # 1x1 conv to adjust channel size without changing spatial dimensions\n",
    "\n",
    "    def forward(self, inputs, timestamps, state):\n",
    "        h, c = state\n",
    "        #print(\"inputs: \", inputs.shape)\n",
    "        combined = torch.cat([inputs, h], dim=1)  # Combining input and hidden state for convolution\n",
    "\n",
    "        # Compute gates and candidate memory cell with convolutional operations\n",
    "        i = torch.sigmoid(self.W_i(combined))\n",
    "        f = torch.sigmoid(self.W_f(combined))\n",
    "        o = torch.sigmoid(self.W_o(combined))\n",
    "        c_hat = torch.tanh(self.W_c(combined))\n",
    "\n",
    "        # Time decay mechanism\n",
    "        c_s = torch.tanh(self.W_d(c))\n",
    "        decay = torch.exp(-timestamps).unsqueeze(1).unsqueeze(2).unsqueeze(3)  # Reshape decay to [batch_size, 1, 1, 1] so that it can be multiplied\n",
    "        decay = decay.expand_as(c_s)  # Ensure decay is broadcastable to the shape of c_s\n",
    "\n",
    "        c_l = c - c_s\n",
    "        c_star = c_l + c_s\n",
    "\n",
    "        # Current memory cell and hidden state\n",
    "        c = f * c_star + i * c_hat\n",
    "        h = o * torch.tanh(c)\n",
    "        print('h:, ', h.shape)\n",
    "\n",
    "        # Transform output to target image size\n",
    "        output = self.output_transform(h)  # Transform the hidden state to match the target image size\n",
    "\n",
    "        if self.cuda_flag:\n",
    "            output = output.cuda()\n",
    "\n",
    "        return output, (h, c)  # Return both the transformed output and the raw hidden states for potential further use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion, device, max_batches=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batches_processed = 0\n",
    "\n",
    "\n",
    "    for images, data in data_loader:\n",
    "        if max_batches is not None and batches_processed >= max_batches:\n",
    "            break  # Stop after processing max_batches\n",
    "        # Extract relative_time for all items in the batch and convert to integers\n",
    "        relative_times = [list(map(int, data_sequence['relative_time'])) for data_sequence in data]\n",
    "\n",
    "        # Convert the list of lists into a 2D tensor of shape (batch_size, sequence_length)\n",
    "        timestamps = torch.tensor(relative_times, dtype=torch.float32).to(device)\n",
    "\n",
    "        images = images.to(device)\n",
    "        #print(timestamps.shape)\n",
    "\n",
    "        # Input is all but the last image in the sequence\n",
    "\n",
    "        input_images = images[:-1, :, :, :, :]\n",
    "        print(\"input_images:\", input_images.shape)\n",
    "        input_timestamps = timestamps[:, :-1]\n",
    "        #print(input_timestamps.shape)\n",
    "\n",
    "        # Target is the last image in the sequence for every sequence in batch\n",
    "        target_image = images[-1, :, :, :, :]\n",
    "        print(\"target imaages:\", target_image.shape)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize hidden and cell states to None for the start of the sequence\n",
    "        # Inside the train_epoch and evaluate_epoch functions, before the loop over t\n",
    "        seq_len, batch_size, _, height, width = input_images.size()\n",
    "        #print(batch_size, seq_len)\n",
    "        hidden_channels = model.hidden_channels\n",
    "\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h = torch.zeros(batch_size, hidden_channels, height, width, device=device)\n",
    "        c = torch.zeros(batch_size, hidden_channels, height, width, device=device)\n",
    "        print(\"c\", c.shape)\n",
    "\n",
    "        state = (h, c)\n",
    "        \n",
    "        # Process the sequence through the ConvTLSTMCell. Go element by element from each batch in parallel.\n",
    "        for t in range(input_images.size(0)):\n",
    "            img_t = input_images[t, :, :, :, :]  # Get each image from each moment in time from batch\n",
    "            print(\"img_t\", img_t.shape)\n",
    "            print(input_timestamps[:, t]) # All the timestamps from the same index from each batch in parallel\n",
    "            # Update the state with each timestep; output is only used at the last timestep\n",
    "            output, state = model(img_t, input_timestamps[:, t], state)\n",
    "\n",
    "        # The final output is the prediction for the last image in the sequence for the whole batch\n",
    "        # of size batch_size, 1, 366, 366.\n",
    "        predicted_image = output\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predicted_image, target_image)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batches_processed += 1\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_epoch(model, data_loader, criterion, device, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batches_processed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, data in data_loader:\n",
    "            if max_batches is not None and batches_processed >= max_batches:\n",
    "                break  # Stop after processing max_batches\n",
    "            \n",
    "            # Extract relative_time for all items in the batch and convert to integers\n",
    "            relative_times = [list(map(int, data_sequence['relative_time'])) for data_sequence in data]\n",
    "\n",
    "            # Convert the list of lists into a 2D tensor of shape (batch_size, sequence_length)\n",
    "            timestamps = torch.tensor(relative_times, dtype=torch.float32).to(device)\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            input_images = images[:-1, :, :, :, :]\n",
    "            input_timestamps = timestamps[:, :-1]\n",
    "            target_image = images[-1, :, :, :, :]\n",
    "\n",
    "            # Initialize hidden and cell states to None for the start of the sequence\n",
    "            # Inside the train_epoch and evaluate_epoch functions, before the loop over t\n",
    "            seq_len, batch_size, _, height, width = input_images.size() \n",
    "            hidden_channels = model.hidden_channels\n",
    "\n",
    "            # Initialize hidden and cell states with zeros\n",
    "            h = torch.zeros(seq_len, hidden_channels, height, width, device=device)\n",
    "            c = torch.zeros(seq_len, hidden_channels, height, width, device=device)\n",
    "\n",
    "            state = (h, c)\n",
    "\n",
    "            # Process the sequence through the ConvTLSTMCell\n",
    "            for t in range(input_images.size(1)):\n",
    "                img_t = input_images[:, t, :, :, :]  # Add channel dimension\n",
    "                output, state = model(img_t, input_timestamps[t, :], state)\n",
    "\n",
    "            predicted_image = output\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predicted_image, target_image)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batches_processed += 1\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_combined(model, train_loader, val_loader, lr, epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up the optimizer and the loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()  # Mean Squared Error loss\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.6)  # TODO: Adjust this\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, max_batches=5)\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        val_loss = evaluate_epoch(model, val_loader, criterion, device, max_batches=5)\n",
    "\n",
    "        # Scheduler step (if using learning rate decay)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch + 1}/{epochs}: Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save model checkpoints periodically or based on certain conditions, e.g., every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f'ConvTLSTM_checkpoint_epoch_{epoch + 1}.pth')\n",
    "            print(f'Model checkpoint saved at epoch {epoch + 1}')\n",
    "\n",
    "    # Optionally, return the model if you need to use it immediately after training\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "sequence_length = 10  # Length of each sequence\n",
    "batch_size = 6  # Number of sequences per batch\n",
    "stride = 1  # Overlap between sequences\n",
    "storm_sampler = GroupedStormSampler(dataset)\n",
    "\n",
    "sequential_batch_sampler = StormBatchSampler(\n",
    "    data_source=dataset,\n",
    "    sequence_length=sequence_length,\n",
    "    batch_size=batch_size,\n",
    "    stride=stride,\n",
    "    drop_last=True \n",
    ")\n",
    "\n",
    "storm_data_loader = DataLoader(dataset, batch_sampler=sequential_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_images: torch.Size([9, 6, 1, 366, 366])\n",
      "target imaages: torch.Size([6, 1, 366, 366])\n",
      "c torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([    0.,  1801.,  3600.,  5400.,  7200., 10802.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([ 1801.,  3600.,  5400.,  7200., 10802., 12602.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([ 3600.,  5400.,  7200., 10802., 12602., 14402.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([ 5400.,  7200., 10802., 12602., 14402., 16202.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([ 7200., 10802., 12602., 14402., 16202., 18002.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([10802., 12602., 14402., 16202., 18002., 21602.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([12602., 14402., 16202., 18002., 21602., 23402.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([14402., 16202., 18002., 21602., 23402., 25202.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n",
      "img_t torch.Size([6, 1, 366, 366])\n",
      "tensor([16202., 18002., 21602., 23402., 25202., 27002.])\n",
      "h:,  torch.Size([6, 8, 366, 366])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m storm_data_loader  \u001b[38;5;66;03m# In practice, you should have a separate validation set\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Call the training routine\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_combined(model, train_loader, val_loader, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device))\n",
      "Cell \u001b[0;32mIn[9], line 135\u001b[0m, in \u001b[0;36mtrain_combined\u001b[0;34m(model, train_loader, val_loader, lr, epochs, device)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 135\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion, device, max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# Evaluation phase\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[9], line 63\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, criterion, device, max_batches)\u001b[0m\n\u001b[1;32m     60\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     66\u001b[0m batches_processed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ese-msc/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ese-msc/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ConvTLSTMCell(input_channels=1, hidden_channels=8, kernel_size=3, output_channels=1, cuda_flag=False)\n",
    "\n",
    "train_loader = storm_data_loader\n",
    "val_loader = storm_data_loader  \n",
    "# TODO: Validation set\n",
    "\n",
    "# Call the training routine\n",
    "train_combined(model, train_loader, val_loader, lr=0.001, epochs=10, device=torch.device(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ese-msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
